{
    "uuid": "89c6eac2-d653-4cce-acea-88a8d6d8d147",
    "name": "dclm_baseline_subsample_12b_tokenized",
    "creation_date": "2024_08_10-17_27_03",
    "dataset_url": "/juice5/scr5/yjruan/dataset/dclm/data/tokenized/dclm_baseline_subsample_12b",
    "manifest_url": "/juice5/scr5/yjruan/dataset/dclm/data/tokenized/dclm_baseline_subsample_12b/manifest.jsonl",
    "sources": [
        {
            "uuid": "e025a6bc-2c45-44b8-a92c-531515d8dba8",
            "name": "dclm_baseline_subsample_12b"
        }
    ],
    "tokenized": true,
    "tokenizer": "EleutherAI/gpt-neox-20b",
    "num_tokens": 11444328876,
    "size": null,
    "dcnlp_commit_hash": "cf8c6f52bb6609567df5eaa3daecacc1cc99c7e3",
    "dcnlp_diff": "diff --git a/ray_processing/tokenize_shuffle.py b/ray_processing/tokenize_shuffle.py\nindex e98b168..5b5eac7 100644\n--- a/ray_processing/tokenize_shuffle.py\n+++ b/ray_processing/tokenize_shuffle.py\n@@ -21,7 +21,7 @@ def add_tokenize_shuffle_args(parser):\n     parser.add_argument(\"--wds_chunk_size\", type=int, default=8192)\n     parser.add_argument(\"--seed\", type=int, default=42)\n     parser.add_argument(\"--subset\", type=int, default=None)\n-    parser.add_argument(\"--ray_address\", type=str, default=None)\n+    parser.add_argument(\"--ray_address\", type=str, default=\"localhost:6379\")\n     parser.add_argument(\"--force_parallelism\", type=int, default=None)\n     parser.add_argument(\"--no_shuffle\", action=\"store_true\")\n     parser.add_argument(\"--do_sample\", action=\"store_true\")\n@@ -34,6 +34,7 @@ def add_tokenize_shuffle_args(parser):\n     parser.add_argument(\n         \"--suffixes\", nargs=\"+\", default=[\".jsonl\", \".jsonl.gz\", \".jsonl.zst\", \".jsonl.zstd\", \".tar\", \".tar.gz\"]\n     )\n+    parser.add_argument(\"--allow_imbalanced_write\", action=\"store_true\")\n \n     # Args specific to dcnlp pipeline (as opposed to tokenize_shuffle)\n     DCNLP_ARGS = [\n@@ -60,7 +61,8 @@ def add_tokenize_shuffle_args(parser):\n \n def main(args, dcnlp_arg_names):\n     # Before proceeding with tokenization, make sure that an existing json won't be overwritten\n-    json_path = f\"exp_data/datasets/tokenized/{args.readable_name}.json\"\n+    # json_path = f\"exp_data/datasets/tokenized/{args.readable_name}.json\"\n+    json_path = os.path.join(os.path.dirname(args.source_ref_paths[0]), f\"{args.readable_name}.json\")\n     if not args.overwrite:\n         assert not os.path.exists(\n             json_path\n@@ -84,7 +86,7 @@ def main(args, dcnlp_arg_names):\n         str(i)\n         for k, v in vars(args).items()\n         for i in [f\"--{k}\", v]\n-        if k not in dcnlp_arg_names and v and k != \"suffixes\"\n+        if k not in dcnlp_arg_names and v and k != \"suffixes\" and k != \"allow_imbalanced_write\"\n     ]\n \n     tokenize_shuffle_args.append(\"--suffixes\")\n@@ -97,6 +99,9 @@ def main(args, dcnlp_arg_names):\n     if args.no_shuffle:\n         tokenize_shuffle_args.append(\"--no_shuffle\")\n \n+    if args.allow_imbalanced_write:\n+        tokenize_shuffle_args.append(\"--allow_imbalanced_write\")\n+\n     tokenize_shuffle.main(tokenize_shuffle_args)\n \n     dataset_json = generate_tokenized_dataset_json(args, source_refs)\n@@ -107,7 +112,7 @@ def main(args, dcnlp_arg_names):\n     if out_json_path.startswith(\"s3://\"):\n         os.system(f\"aws s3 cp {json_path} {out_json_path}\")\n     else:\n-        os.system(f\"mv {json_path} {out_json_path}\")\n+        os.system(f\"cp {json_path} {out_json_path}\")\n \n \n if __name__ == \"__main__\":\ndiff --git a/ray_processing/utils.py b/ray_processing/utils.py\nindex efc56be..c43a69e 100644\n--- a/ray_processing/utils.py\n+++ b/ray_processing/utils.py\n@@ -32,7 +32,12 @@ def get_source_ref(source_ref_path):\n \n \n def count_tokens(manifest_url, seqlen=2049):\n-    with S3Path(manifest_url).open(\"r\") as f:\n+    if manifest_url.startswith(\"s3://\"):\n+        manifest_url = S3Path(manifest_url).open(\"r\")\n+    else:\n+        manifest_url = open(manifest_url, \"r\")\n+    \n+    with manifest_url as f:\n         manifest = [json.loads(line) for line in f]\n     num_tokens = sum(int(line[\"num_sequences\"]) for line in manifest) * seqlen\n     return num_tokens\n@@ -46,6 +51,13 @@ def get_s3_dir_size(dataset_path):\n     return total_size\n \n \n+def get_dir_size(dataset_path):\n+    if dataset_path.startswith(\"s3://\"):\n+        return get_s3_dir_size(dataset_path)\n+    else:\n+        return None  # no count now\n+\n+\n def get_git_info():\n     repo = git.Repo(search_parent_directories=True)\n     dcnlp_commit_hash = repo.head.object.hexsha\n@@ -57,13 +69,6 @@ def generate_untokenized_dataset_json(args, source_refs, base_output_path, data_\n     sources = [{\"uuid\": s[\"uuid\"], \"name\": s[\"name\"]} for s in source_refs] if source_refs else []\n     dcnlp_commit_hash, dcnlp_diff = get_git_info()\n \n-    use_s3 = args.output_dir.startswith(\"s3://\")\n-    if use_s3:\n-        size = get_s3_dir_size(args.output_dir)\n-    else:\n-        size = None\n-    \n-\n     dataset_json = {\n         \"uuid\": str(uuid.uuid4().__str__()),\n         \"name\": args.readable_name,\n@@ -74,7 +79,7 @@ def generate_untokenized_dataset_json(args, source_refs, base_output_path, data_\n         \"tokenized\": False,\n         \"tokenizer\": None,\n         \"num_tokens\": None,\n-        \"size\": size,\n+        \"size\": get_dir_size(args.output_dir),\n         \"dcnlp_commit_hash\": dcnlp_commit_hash,\n         \"dcnlp_diff\": dcnlp_diff,\n         \"data_key\": data_key,\n@@ -106,7 +111,7 @@ def generate_tokenized_dataset_json(args, source_refs, data_key=\"json.gz\"):\n         \"tokenized\": True,\n         \"tokenizer\": args.tokenizer,\n         \"num_tokens\": count_tokens(manifest_url, args.seqlen + 1),\n-        \"size\": get_s3_dir_size(args.output),\n+        \"size\": get_dir_size(args.output),\n         \"dcnlp_commit_hash\": dcnlp_commit_hash,\n         \"dcnlp_diff\": dcnlp_diff,\n         \"data_key\": data_key,\ndiff --git a/setup.py b/setup.py\nindex ffd4b7e..beacf79 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -215,23 +215,23 @@ setup(\n     },\n )\n \n-setup(\n-    name='your_package',\n-    version='0.1.0',\n-    description='Your package description',\n-    packages=find_packages(),\n-    install_requires=required,\n-    extras_require={\n-        'baselines': ['some_package>=1.0.0'],\n-        'training': ['tensorflow>=2.0', 'keras'],\n-        'eval': ['scikit-learn'],\n-        'dev': ['pytest', 'sphinx']\n-    },\n-    cmdclass={\n-        'install': DownloadAssetsCommand,\n-        'install_baselines': BaselineInstall,\n-        'install_training': TrainingInstall,\n-        'install_eval': EvalInstall,\n-        'install_dev': DevInstall\n-    },\n-)\n+# setup(\n+#     name='your_package',\n+#     version='0.1.0',\n+#     description='Your package description',\n+#     packages=find_packages(),\n+#     install_requires=required,\n+#     extras_require={\n+#         'baselines': ['some_package>=1.0.0'],\n+#         'training': ['tensorflow>=2.0', 'keras'],\n+#         'eval': ['scikit-learn'],\n+#         'dev': ['pytest', 'sphinx']\n+#     },\n+#     cmdclass={\n+#         'install': DownloadAssetsCommand,\n+#         'install_baselines': BaselineInstall,\n+#         'install_training': TrainingInstall,\n+#         'install_eval': EvalInstall,\n+#         'install_dev': DevInstall\n+#     },\n+# )",
    "data_key": "json.gz",
    "sampling_yaml": null
}